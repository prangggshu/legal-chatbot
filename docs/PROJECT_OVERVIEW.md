# Legal AI Chatbot - Project Overview

## Executive Summary

The Legal AI Chatbot is an intelligent document analysis system that combines:
- **Retrieval-Augmented Generation (RAG)** for accurate legal Q&A
- **Local authentication** (no external databases)
- **Multi-tier LLM architecture** (local + cloud fallback)
- **Automated risk detection** and document summarization
- **Semantic document search** using vector embeddings

Target Users: Lawyers, legal teams, contract reviewers, compliance officers

---

## üéØ Core Features

### 1. Document Intelligence
- **Upload PDFs**: Process legal contracts and documents
- **Semantic Search**: Find relevant clauses using embeddings
- **Risk Detection**: Automatically identify problematic clauses
- **Summarization**: Get document overview in seconds
- **Multi-level Chunking**: Break documents into meaningful segments

### 2. Q&A System
- **RAG-based Answers**: Retrieve relevant clauses, then generate answers
- **Spell Correction**: Handle typos and misspellings
- **Legal References**: Parse "Section X of Act Y" directly
- **Fuzzy Matching**: Find similar questions even with different wording
- **Fallback Intelligence**: Use Gemini API for out-of-knowledge-base questions

### 3. Security
- **Local Authentication**: No external database, PBKDF2 hashing
- **Token-based Auth**: HMAC-signed tokens with expiration
- **Session Persistence**: Remember login across browser refreshes
- **Secure Storage**: Credentials stored locally with salt + hash

### 4. Analysis & Insights
- **Risk Scoring**: High/Medium/Low classification
- **Clause Extraction**: Identify key terms and conditions
- **Metadata Tracking**: Know document source and retrieval confidence
- **Confidence Scores**: Understand answer reliability

---

## üèóÔ∏è System Architecture

### High-Level Data Flow

```
User ‚îÄ‚îÄ‚Üí Frontend (React) ‚îÄ‚îÄ‚Üí API (FastAPI) ‚îÄ‚îÄ‚Üí Backend Services
                                  ‚Üì
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ  RAG Engine         ‚îÇ
                        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                        ‚îÇ ‚ñ™ FAISS Index       ‚îÇ
                        ‚îÇ ‚ñ™ Embeddings        ‚îÇ
                        ‚îÇ ‚ñ™ Vector Search     ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚Üì
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ  LLM Routing        ‚îÇ
                        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                        ‚îÇ ‚ñ™ Ollama (local)    ‚îÇ
                        ‚îÇ ‚ñ™ Gemini (fallback) ‚îÇ
                        ‚îÇ ‚ñ™ Timeout handling  ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚Üì
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ  Analysis Engine    ‚îÇ
                        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                        ‚îÇ ‚ñ™ Risk Detection    ‚îÇ
                        ‚îÇ ‚ñ™ Summarization     ‚îÇ
                        ‚îÇ ‚ñ™ Metadata Tracking ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Component Responsibilities

| Component | Purpose | Technology |
|-----------|---------|-----------|
| **Frontend** | User interface | React, TypeScript, Tailwind |
| **API Layer** | HTTP endpoints | FastAPI, Pydantic |
| **RAG Engine** | Document search | FAISS, Sentence Transformers |
| **Auth System** | User management | PBKDF2, HMAC, JSON store |
| **LLM Layer** | Answer generation | Ollama, Google Gemini |
| **Storage** | Persistence | FAISS index, JSON files |

---

## üìä User Journey

### Journey 1: Upload & Analyze Document

```
1. User logs in
   ‚îî‚îÄ Email/password ‚Üí Auth API
   ‚îî‚îÄ Receive bearer token
   ‚îî‚îÄ Token stored in localStorage

2. User uploads PDF
   ‚îî‚îÄ POST /upload with file
   ‚îî‚îÄ Backend extracts text, chunks it
   ‚îî‚îÄ Embeds chunks, adds to FAISS index
   ‚îî‚îÄ Returns: chunks created/added count

3. User requests analysis
   ‚îî‚îÄ GET /analyze
   ‚îî‚îÄ Backend scans all chunks for risk
   ‚îî‚îÄ Returns: risk summary + detailed sections

4. User sees results
   ‚îî‚îÄ Dashboard shows risks by severity
   ‚îî‚îÄ Clauses highlighted with explanations
```

### Journey 2: Ask Question

```
1. User enters query
   ‚îî‚îÄ "What is the termination clause?"

2. Query Processing
   ‚îú‚îÄ Spell correction
   ‚îú‚îÄ Legal reference extraction (Section X check)
   ‚îú‚îÄ Exact question match (fuzzy)
   ‚îî‚îÄ If no match: proceed to semantic search

3. Semantic Search
   ‚îú‚îÄ Embed query (384-dim vector)
   ‚îú‚îÄ FAISS search top 50 chunks
   ‚îú‚îÄ Score by: embedding (82%) + keywords (15%) + source (3%)
   ‚îî‚îÄ Return top match

4. LLM Generation
   ‚îú‚îÄ Try Ollama (4s timeout)
   ‚îú‚îÄ If timeout/error: Use Gemini API
   ‚îî‚îÄ Return generated answer

5. Risk Detection
   ‚îî‚îÄ Scan clause for risk keywords
   ‚îî‚îÄ Assign risk: High/Medium/Low

6. Response Format
   ‚îî‚îÄ JSON with answer, source, confidence, risk
```

### Journey 3: Get Document Summary

```
1. User requests summary
   ‚îî‚îÄ GET /summarize

2. Full Document Processing
   ‚îî‚îÄ Extract all text from uploaded PDF
   ‚îî‚îÄ Pass to Gemini API

3. Gemini Analysis
   ‚îî‚îÄ Identified: parties, purpose, scope, terms, obligations
   ‚îî‚îÄ Generate 2-3 paragraph summary

4. User Views Summary
   ‚îî‚îÄ Key points displayed
   ‚îî‚îÄ Can drill down to full scan
```

---

## üîç Technical Decisions

### Why FAISS for Vector Search?

- ‚úÖ **Fast L2 distance search** (50ms for 50K vectors)
- ‚úÖ **Persistent storage** (serialize/deserialize)
- ‚úÖ **No external service** (runs locally)
- ‚úÖ **Scalable** (can handle 1M+ vectors)
- ‚ùå Trade-off: No approximate search needed yet

### Why Multi-Tier LLM?

- ‚úÖ **Privacy**: Ollama stays local, no data to cloud
- ‚úÖ **Cost**: Free after initial model download
- ‚úÖ **Speed**: 1-3 seconds for small queries
- ‚ö†Ô∏è **Reliability**: Falls back to Gemini if timeout
- ‚úÖ **Fallback**: Gemini handles complex queries

### Why Local Authentication?

- ‚úÖ **No database**, just JSON file
- ‚úÖ **PBKDF2 hashing**, 120K iterations
- ‚úÖ **Stateless tokens**, can add to any request
- ‚úÖ **Simple deployment**, no DB migrations
- ‚ùå Trade-off: No token revocation (no blacklist)

### Why Multi-Level Chunking?

- ‚úÖ **Semantic coherence**: Chunks follow document structure
- ‚úÖ **Granular analysis**: Risk at clause level, not whole doc
- ‚úÖ **Better retrieval**: Smaller vectors, more precise search
- ‚úÖ **Flexible**: ARTICLE ‚Üí clause ‚Üí sliding window fallback
- ‚úÖ **Constraint**: 50-1000 words ensures meaningful content

---

## üìà Data Flow Details

### Query to Answer Pipeline

```
Input: "What is liability in this contract?"
   ‚Üì
Step 1: Spell Correct
   "What is liabl..." ‚Üí "What is liability..."
   ‚Üì
Step 2: Extract Legal Reference
   Pattern: (section|clause) + (number) + (act)
   Result: None (not "Section X" pattern)
   ‚Üì
Step 3: Exact Question Match (Fuzzy)
   Search DB: "What is the liability...?"
   Similarity: 0.87 (‚â•0.85 threshold) ‚úì Found!
   ‚Üì
Step 4: (Skip semantic search, already matched)
   Return confidence: 0.99
   ‚Üì
Step 5: LLM Generation (Router)
   Start Ollama (4s timeout)
   ‚Üí If success: return answer
   ‚Üí If timeout: use Gemini API
   ‚Üì
Step 6: Risk Detection
   Scan clause: "liability limited to..."
   Keywords found: "liability"
   Risk Level: Medium
   ‚Üì
Output: JSON Response
{
  "answer": "Liability is limited to...",
  "confidence_score": 0.99,
  "risk_level": "Medium",
  "clause_reference": "Section 8"
}
```

### Document Upload Pipeline

```
Input: contract.pdf (10 MB)
   ‚Üì
Step 1: Extract Text
   Use pdfplumber to read all pages
   Result: Raw text 50K words
   ‚Üì
Step 2: Multi-Level Chunking
   Level 1: Split by ARTICLE/SECTION
   Result: 8 articles
   ‚Üì
   Level 2: Split by numbered clauses (3.14)
   Result: 45 chunks (50-300 words each)
   ‚Üì
   Level 3: Sliding window (if needed)
   Target: 150 words per chunk
   ‚Üì
Step 3: Vector Embedding
   Model: Sentence Transformers (all-MiniLM-L6-v2)
   Per chunk: 45 ‚Üí 45 embeddings (384-dim)
   ‚Üì
Step 4: FAISS Indexing
   Add vectors to IndexFlatL2
   Store chunks in JSON
   ‚Üì
Step 5: Persistence
   Save to disk:
   - legal_qa.index (binary)
   - legal_qa_chunks.json
   - legal_qa_sources.json
   ‚Üì
Output: Chunks Created: 45
        Chunks Added: 42
```

---

## üóÇÔ∏è File Organization

```
legal-chatbot/
‚îú‚îÄ‚îÄ frontend/                     # React application
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.tsx              # Main component (auth + chat)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/          # Reusable components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/            # API integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types.ts             # TypeScript definitions
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ styles/              # CSS (Tailwind)
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îÇ
‚îú‚îÄ‚îÄ backend/                      # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ rag_engine/              # Core modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py              # API endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag_core.py          # Vector search
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth_local.py        # Auth system
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document_processor.py # PDF handling
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_router.py        # LLM selection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini_engine.py     # Cloud LLM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_engine.py        # Local LLM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ risk_engine.py       # Risk detection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ relevance_reranker.py # Fine-tuning
‚îÇ   ‚îú‚îÄ‚îÄ uploads/                 # User PDFs
‚îÇ   ‚îú‚îÄ‚îÄ vector_db/               # FAISS index
‚îÇ   ‚îú‚îÄ‚îÄ local_users.json         # Auth store
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ
‚îî‚îÄ‚îÄ docs/                         # Documentation (YOU ARE HERE)
    ‚îú‚îÄ‚îÄ README.md                # Documentation hub
    ‚îú‚îÄ‚îÄ PROJECT_OVERVIEW.md      # This file
    ‚îú‚îÄ‚îÄ SETUP_GUIDE.md
    ‚îú‚îÄ‚îÄ BACKEND_GUIDE.md
    ‚îú‚îÄ‚îÄ FRONTEND_GUIDE.md
    ‚îú‚îÄ‚îÄ API_REFERENCE.md
    ‚îú‚îÄ‚îÄ ARCHITECTURE.md
    ‚îî‚îÄ‚îÄ TROUBLESHOOTING.md
```

---

## üîÑ Integration Points

### Frontend ‚Üî Backend
- **Protocol**: HTTP/REST
- **Format**: JSON request/response
- **Auth**: Bearer token in header
- **Base URL**: `/api` (proxied by Vite dev server)

### Backend ‚Üî LLM Services
- **Local**: Ollama at `http://localhost:11434`
- **Cloud**: Google Gemini API with `GEMINI_API_KEY`

### Backend ‚Üî Storage
- **Vector DB**: FAISS persistent files
- **Auth**: `local_users.json`
- **Uploads**: `uploads/` directory
- **Metadata**: JSON files in `vector_db/`

---

## üìä Performance Characteristics

| Operation | Time | Notes |
|-----------|------|-------|
| Document Upload (10 MB) | 5-10s | PDF extract + chunk + embed + index |
| Query Processing | ~100ms | Embedding + FAISS search |
| Semantic Search | 50ms | Top 50 candidates |
| Ollama Generation | 1-3s | Local model (llama3) |
| Gemini Generation | 2-4s | Cloud API + network |
| Risk Analysis | 2-5s | Scan all chunks |
| Summarization | 3-8s | Full document ‚Üí Gemini |

---

## üîê Security Architecture

### Authentication Flow

```
Register:
  username + password
    ‚Üì
  Validate (unique, ‚â•6 chars)
    ‚Üì
  Generate salt (16 random hex bytes)
    ‚Üì
  Hash: PBKDF2(password, salt, 120K iterations)
    ‚Üì
  Store: {salt, hash} in local_users.json
    
Login:
  username + password
    ‚Üì
  Load user from file
    ‚Üì
  Recompute hash: PBKDF2(password, stored_salt)
    ‚Üì
  Compare using constant-time compare
    ‚Üì
  If match: Create token
    Payload: {sub, iat, exp}
    Sign: HMAC-SHA256(payload, secret)
    Token: base64(payload).base64(sig)
    
Verify:
  Extract payload, sig from token
    ‚Üì
  Verify HMAC signature
    ‚Üì
  Check exp > now
    ‚Üì
  Return payload or error
```

---

## üöÄ Deployment Architecture

### Local Development
```
Frontend: Vite dev server (port 3000)
Backend: Uvicorn (port 8000)
Storage: Local filesystem
Auth: local_users.json
```

### Production Ready
```
Frontend: Built React bundle + CDN
Backend: Uvicorn + Gunicorn
Storage: FAISS index + cloud backup
Auth: local_users.json or integrate with DB
```

---

## üìù Next Steps

- **Start Using**: See [USAGE_GUIDE.md](USAGE_GUIDE.md)
- **Set Up Locally**: See [SETUP_GUIDE.md](SETUP_GUIDE.md)
- **Develop Backend**: See [BACKEND_GUIDE.md](BACKEND_GUIDE.md)
- **Develop Frontend**: See [FRONTEND_GUIDE.md](FRONTEND_GUIDE.md)
- **Fix Issues**: See [TROUBLESHOOTING.md](TROUBLESHOOTING.md)

---

**Version:** 1.0  
**Last Updated:** February 24, 2026
